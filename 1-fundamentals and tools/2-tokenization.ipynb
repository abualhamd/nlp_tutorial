{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\benha\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "d\n",
      "1\n",
      "-----------------------\n",
      "Tesla\n",
      "Xxxxx\n",
      "Tesla\n",
      "-----------------------\n",
      "is\n",
      "xx\n",
      "be\n",
      "-----------------------\n",
      "looking\n",
      "xxxx\n",
      "look\n",
      "-----------------------\n",
      "at\n",
      "xx\n",
      "at\n",
      "-----------------------\n",
      "buying\n",
      "xxxx\n",
      "buy\n",
      "-----------------------\n",
      "U.S.\n",
      "X.X.\n",
      "U.S.\n",
      "-----------------------\n",
      "startUp\n",
      "xxxxXx\n",
      "startUp\n",
      "-----------------------\n",
      "for\n",
      "xxx\n",
      "for\n",
      "-----------------------\n",
      "$\n",
      "$\n",
      "$\n",
      "-----------------------\n",
      "6\n",
      "d\n",
      "6\n",
      "-----------------------\n",
      "millions\n",
      "xxxx\n",
      "million\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('1 Tesla is looking at buying U.S. startUp for $6 millions')\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    # print(token.text)\n",
    "    print(token.shape_)\n",
    "    # print(token.is_alpha)\n",
    "    # print(token.is_stop)\n",
    "    print(token.lemma_)\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, Tesla, is, looking, at, buying, U.S., startUp, for, $, 6, millions]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'bad apple death'\n",
    "\n",
    "doc2 = nlp(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "apple\n",
      "death\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[bad, apple, death]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [token for token in doc2]\n",
    "my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "xx\n",
      "we\n",
      "____________________\n",
      "'re\n",
      "'xx\n",
      "be\n",
      "____________________\n",
      "moving\n",
      "xxxx\n",
      "move\n",
      "____________________\n",
      "to\n",
      "xx\n",
      "to\n",
      "____________________\n",
      "L.A.\n",
      "X.X.\n",
      "L.A.\n",
      "____________________\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp('we\\'re moving to L.A.')\n",
    "for token in doc3:\n",
    "    print(token)\n",
    "    print(token.shape_)\n",
    "    print(token.lemma_)\n",
    "    print('____________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "look\n",
      "at\n",
      "blabla@gmail.com\n",
      "at\n",
      "6:30\n",
      "am\n",
      "and\n",
      "open\n",
      "www.youtube.com\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp('look at blabla@gmail.com at 6:30 am and open www.youtube.com')\n",
    "\n",
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'apple', 'death']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['look',\n",
       " 'at',\n",
       " 'blabla',\n",
       " '@',\n",
       " 'gmail.com',\n",
       " 'at',\n",
       " '6:30',\n",
       " 'am',\n",
       " 'and',\n",
       " 'open',\n",
       " 'www.youtube.com']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('look at blabla@gmail.com at 6:30 am and open www.youtube.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"'re\", 'moving', 'to', 'L.A', '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('we\\'re moving to L.A.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "the\n",
      "mathematical\n",
      "theory\n",
      "of\n",
      "artificial\n",
      "neural\n",
      "networks\n",
      ",\n",
      "universal\n",
      "approximation\n",
      "theorems\n",
      "are\n",
      "results[1][2\n",
      "]\n",
      "that\n",
      "establish\n",
      "the\n",
      "density\n",
      "of\n",
      "an\n",
      "algorithmically\n",
      "generated\n",
      "class\n",
      "of\n",
      "functions\n",
      "within\n",
      "a\n",
      "given\n",
      "function\n",
      "space\n",
      "of\n",
      "interest\n",
      ".\n",
      "Typically\n",
      ",\n",
      "these\n",
      "results\n",
      "concern\n",
      "the\n",
      "approximation\n",
      "capabilities\n",
      "of\n",
      "the\n",
      "feedforward\n",
      "architecture\n",
      "on\n",
      "the\n",
      "space\n",
      "of\n",
      "continuous\n",
      "functions\n",
      "between\n",
      "two\n",
      "Euclidean\n",
      "spaces\n",
      ",\n",
      "and\n",
      "the\n",
      "approximation\n",
      "is\n",
      "with\n",
      "respect\n",
      "to\n",
      "the\n",
      "compact\n",
      "convergence\n",
      "topology\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('In the mathematical theory of artificial neural networks, universal approximation theorems are results[1][2] that establish the density of an algorithmically generated class of functions within a given function space of interest. Typically, these results concern the approximation capabilities of the feedforward architecture on the space of continuous functions between two Euclidean spaces, and the approximation is with respect to the compact convergence topology.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the mathematical theory of artificial neural networks, universal approximation theorems are results[1][2] that establish the density of an algorithmically generated class of functions within a given function space of interest.\n",
      "[]\n",
      "()\n",
      "_______________________________\n",
      "Typically, these results concern the approximation capabilities of the feedforward architecture on the space of continuous functions between two Euclidean spaces, and the approximation is with respect to the compact convergence topology.\n",
      "[two, Euclidean]\n",
      "(is,)\n",
      "_______________________________\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    print(sent.ents)\n",
    "    print(sent.conjuncts)\n",
    "    print('_______________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].is_sent_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Typically]\n"
     ]
    }
   ],
   "source": [
    "starts = [start[0] for start in doc.sents]\n",
    "print(starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right; Leadership is doing the right thing.\"\n",
      "-some random dude\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp('\"Management is doing things right; Leadership is doing the right thing.\" -some random dude')\n",
    "\n",
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'custom_boundaries_component',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_boundaries_component\")\n",
    "def custom_boundaries(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if(token.text == ';'):\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"custom_boundaries_component\", before='parser')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right;\n",
      "Leadership is doing the right thing.\"\n",
      "-some random dude\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp('\"Management is doing things right; Leadership is doing the right thing.\" -some random dude')\n",
    "\n",
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the mathematical theory of artificial neural networks, universal approximation theorems are results[1][2] that establish the density of an algorithmically generated class of functions within a given function space of interest.\n",
      "_____________________________\n",
      "Typically, these results concern the approximation capabilities of the feedforward architecture on the space of continuous functions between two Euclidean spaces, and the approximation is with respect to the compact convergence topology.\n",
      "_____________________________\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "for sent in sent_tokenize('In the mathematical theory of artificial neural networks, universal approximation theorems are results[1][2] that establish the density of an algorithmically generated class of functions within a given function space of interest. Typically, these results concern the approximation capabilities of the feedforward architecture on the space of continuous functions between two Euclidean spaces, and the approximation is with respect to the compact convergence topology.'):\n",
    "    print(sent)\n",
    "    print('_____________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right; Leadership is doing the right thing.\"\n",
      "-some random dude\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize('\"Management is doing things right; Leadership is doing the right thing.\" -some random dude'):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "However, there are also a variety of results between non-Euclidean spaces[3] and other commonly used architectures and, more generally, algorithmically generated sets of functions, such as the convolutional neural network (CNN) architecture,[4][5] radial basis-functions,[6] or neural networks with specific properties.[7] Most universal approximation theorems can be parsed into two classes. The first quantifies the approximation capabilities of neural networks with an arbitrary number of artificial neurons (\"arbitrary width\" case) and the second focuses on the case with an arbitrary number of hidden layers, each containing a limited number of artificial neurons (\"arbitrary depth\" case).\n",
    "Universal approximation theorems imply that neural networks can represent a wide variety of interesting functions when given appropriate weights. On the other hand, they typically do not provide a construction for the weights, but merely state that such a construction is possible. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "However, there are also a variety of results between non-Euclidean spaces[3] and other commonly used architectures and, more generally, algorithmically generated sets of functions, such as the convolutional neural network (CNN) architecture,[4][5] radial basis-functions,[6] or neural networks with specific properties.\n",
      "________________\n",
      "[7] Most universal approximation theorems can be parsed into two classes.\n",
      "________________\n",
      "The first quantifies the approximation capabilities of neural networks with an arbitrary number of artificial neurons (\"arbitrary width\" case) and the second focuses on the case with an arbitrary number of hidden layers, each containing a limited number of artificial neurons (\"arbitrary depth\" case).\n",
      "________________\n",
      "Universal approximation theorems imply that neural networks can represent a wide variety of interesting functions when given appropriate weights.\n",
      "________________\n",
      "On the other hand, they typically do not provide a construction for the weights, but merely state that such a construction is possible.\n",
      "________________\n"
     ]
    }
   ],
   "source": [
    "from nltk import PunktSentenceTokenizer\n",
    "\n",
    "for sent in PunktSentenceTokenizer(text).tokenize(text):\n",
    "    print(sent)\n",
    "    print('________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "However, there are also a variety of results between non-Euclidean spaces[3] and other commonly used architectures and, more generally, algorithmically generated sets of functions, such as the convolutional neural network (CNN) architecture,[4][5] radial basis-functions,[6] or neural networks with specific properties.\n",
      "________________\n",
      "[7] Most universal approximation theorems can be parsed into two classes.\n",
      "________________\n",
      "The first quantifies the approximation capabilities of neural networks with an arbitrary number of artificial neurons (\"arbitrary width\" case) and the second focuses on the case with an arbitrary number of hidden layers, each containing a limited number of artificial neurons (\"arbitrary depth\" case).\n",
      "________________\n",
      "Universal approximation theorems imply that neural networks can represent a wide variety of interesting functions when given appropriate weights.\n",
      "________________\n",
      "On the other hand, they typically do not provide a construction for the weights, but merely state that such a construction is possible.\n",
      "________________\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(text):\n",
    "    print(sent)\n",
    "    print('________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e59a91fa698e4668adac386abeaaa9e787f2efa0af0899911c24b101ccac97b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
